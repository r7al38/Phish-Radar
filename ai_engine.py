import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, pipeline
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
import requests
import re
from textblob import TextBlob
from langdetect import detect, LangDetectException
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ NLTK (Ù„Ø£ÙˆÙ„ Ù…Ø±Ø© ÙÙ‚Ø·)
try:
    nltk.data.find('sentiment/vader_lexicon')
except:
    nltk.download('vader_lexicon')

class AdvancedAIEngine:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.vectorizer = None
        self.classifier = None
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        self.load_models()
    
    def load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©"""
        try:
            # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©/Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            self.tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-uncased")
            self.model = AutoModel.from_pretrained("bert-base-multilingual-uncased")
            
            # ØªØ­Ù…ÙŠÙ„ Ù…ØµÙ†Ù Ù…Ø­Ù„ÙŠ (Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯)
            try:
                self.vectorizer = joblib.load('models/tfidf_vectorizer.pkl')
                self.classifier = joblib.load('models/phishing_classifier.pkl')
            except:
                print("Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ù„ÙŠØ© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©")
                
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
    
    def extract_advanced_features(self, url, html_content=None, text_content=None):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ù…ØªÙ‚Ø¯Ù…Ø©"""
        features = {}
        
        # 1. Ø®ØµØ§Ø¦Øµ URL Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        features.update(self._extract_url_features(url))
        
        # 2. Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Øµ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
        if text_content:
            features.update(self._extract_text_features(text_content))
        
        # 3. Ø®ØµØ§Ø¦Øµ HTML (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
        if html_content:
            features.update(self._extract_html_features(html_content))
        
        # 4. Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        features.update(self._extract_ai_features(url, text_content))
        
        return features
    
    def _extract_url_features(self, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ URL"""
        features = {}
        
        # ØªØ­Ù„ÙŠÙ„ URL
        features['url_length'] = len(url)
        features['num_dots'] = url.count('.')
        features['num_hyphens'] = url.count('-')
        features['num_underscore'] = url.count('_')
        features['num_slash'] = url.count('/')
        features['has_https'] = 1 if url.startswith('https') else 0
        features['has_ip'] = 1 if re.match(r'\d+\.\d+\.\d+\.\d+', url) else 0
        
        # ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©
        suspicious_words = ['login', 'verify', 'account', 'bank', 'secure', 'update',
                           'confirm', 'password', 'credential', 'urgent', 'immediately']
        features['suspicious_words_count'] = sum(1 for word in suspicious_words if word in url.lower())
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø­Ø±Ù
        features['digit_ratio'] = sum(c.isdigit() for c in url) / len(url) if url else 0
        features['letter_ratio'] = sum(c.isalpha() for c in url) / len(url) if url else 0
        
        return features
    
    def _extract_text_features(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù†Øµ"""
        features = {}
        
        if not text:
            return features
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        sentiment_scores = self.sentiment_analyzer.polarity_scores(text)
        features['sentiment_compound'] = sentiment_scores['compound']
        features['sentiment_positive'] = sentiment_scores['pos']
        features['sentiment_negative'] = sentiment_scores['neg']
        features['sentiment_neutral'] = sentiment_scores['neu']
        
        # ØªØ­Ù„ÙŠÙ„ TextBlob
        try:
            blob = TextBlob(text)
            features['textblob_polarity'] = blob.sentiment.polarity
            features['textblob_subjectivity'] = blob.sentiment.subjectivity
        except:
            features['textblob_polarity'] = 0
            features['textblob_subjectivity'] = 0
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Øµ
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0
        
        # ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø·ÙˆØ§Ø±Ø¦
        urgency_words = ['urgent', 'immediately', 'now', 'quick', 'alert', 'warning',
                        'important', 'action required', 'verify now']
        features['urgency_words_count'] = sum(1 for word in urgency_words if word in text.lower())
        
        return features
    
    def _extract_html_features(self, html):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ HTML"""
        features = {}
        
        # Ø¹Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ±
        features['form_count'] = html.count('<form')
        features['input_count'] = html.count('<input')
        features['password_count'] = html.count('type="password"')
        features['script_count'] = html.count('<script')
        features['link_count'] = html.count('<a href')
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ HTML
        text_length = len(re.sub('<[^<]+?>', '', html))
        features['text_html_ratio'] = text_length / len(html) if html else 0
        
        return features
    
    def _extract_ai_features(self, url, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        features = {}
        
        # ØªØ­Ù„ÙŠÙ„ BERT Ù„Ù„Ù†Øµ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯)
        if text and self.tokenizer and self.model:
            try:
                inputs = self.tokenizer(text[:512], return_tensors="pt", truncation=True, max_length=512)
                with torch.no_grad():
                    outputs = self.model(**inputs)
                embeddings = outputs.last_hidden_state.mean(dim=1).numpy()[0]
                
                # Ø£Ø®Ø° Ø£ÙˆÙ„ 10 Ù‚ÙŠÙ… Ù…Ù† Ø§Ù„Ù€ embeddings ÙƒÙ…ÙŠØ²Ø§Øª
                for i in range(min(10, len(embeddings))):
                    features[f'bert_embedding_{i}'] = embeddings[i]
                    
            except Exception as e:
                print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ BERT: {e}")
        
        # ÙƒØ´Ù Ø§Ù„Ù„ØºØ©
        try:
            if text:
                lang = detect(text)
                features['is_english'] = 1 if lang == 'en' else 0
                features['is_arabic'] = 1 if lang == 'ar' else 0
        except LangDetectException:
            features['is_english'] = 0
            features['is_arabic'] = 0
        
        return features
    
    def predict_phishing(self, features):
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        if not self.classifier:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø¨Ø³ÙŠØ·Ø© Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ù…ØµÙ†Ù Ù…ØªØ§Ø­Ø§Ù‹
            risk_score = self._calculate_risk_score(features)
            return {
                'is_phishing': risk_score > 0.6,
                'confidence': risk_score,
                'risk_level': 'high' if risk_score > 0.7 else 'medium' if risk_score > 0.4 else 'low'
            }
        
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù…ØµÙÙˆÙØ©
            feature_array = np.array([list(features.values())]).reshape(1, -1)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            prediction = self.classifier.predict(feature_array)[0]
            probability = self.classifier.predict_proba(feature_array)[0]
            
            confidence = probability[1] if prediction else probability[0]
            
            return {
                'is_phishing': bool(prediction),
                'confidence': float(confidence),
                'risk_level': 'high' if confidence > 0.8 else 'medium' if confidence > 0.5 else 'low'
            }
            
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")
            risk_score = self._calculate_risk_score(features)
            return {
                'is_phishing': risk_score > 0.6,
                'confidence': risk_score,
                'risk_level': 'high' if risk_score > 0.7 else 'medium' if risk_score > 0.4 else 'low'
            }
    
    def _calculate_risk_score(self, features):
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø© ÙŠØ¯ÙˆÙŠØ§Ù‹"""
        risk_score = 0
        
        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        if features.get('suspicious_words_count', 0) > 3:
            risk_score += 0.3
        
        if features.get('url_length', 0) > 75:
            risk_score += 0.2
        
        if features.get('has_ip', 0) == 1:
            risk_score += 0.3
        
        if features.get('urgency_words_count', 0) > 2:
            risk_score += 0.2
        
        if features.get('sentiment_negative', 0) > 0.5:
            risk_score += 0.1
        
        if features.get('password_count', 0) > 0:
            risk_score += 0.2
        
        return min(risk_score, 1.0)

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø±Ùƒ
if __name__ == "__main__":
    ai_engine = AdvancedAIEngine()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ÙŠ
    test_url = "http://paypal-security-verify.com/login"
    test_text = "Urgent: Your account has been suspended. Verify your credentials immediately."
    
    features = ai_engine.extract_advanced_features(test_url, text_content=test_text)
    prediction = ai_engine.predict_phishing(features)
    
    print("ğŸ” Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
    print(f"Ø§Ù„Ø±Ø§Ø¨Ø·: {test_url}")
    print(f"Ø§Ù„ØªÙ†Ø¨Ø¤: {'ØªØµÙŠØ¯' if prediction['is_phishing'] else 'Ø¢Ù…Ù†'}")
    print(f"Ø§Ù„Ø«Ù‚Ø©: {prediction['confidence']:.2%}")
    print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©: {prediction['risk_level']}")